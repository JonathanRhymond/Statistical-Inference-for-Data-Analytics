---
title: "Stat 3202: Homework 2"
date: "Due Thursday, February 01 by 11:59 pm"
author: "Jonathan Rhymond (rhymond.1)"
output: pdf_document
---

\

Setup:
```{r message=FALSE}
set.seed(3202)
```


\

#### Instructions
- Replace "FirstName LastName (name.n)" above with your information.
- Provide your solutions below in the spaces marked "Solution:".
- Include any R code that you use to answer the questions; if a numeric answer is required, show how you calculated it in R.
- Knit this document to **pdf** and upload both the **pdf** file and your completed **Rmd** file to Carmen
- Make sure your solutions are clean and easy-to-read by
    - formatting all plots to be appropriately sized, with appropriate axis labels.
    - only including R code that is necessary to answer the questions below.
    - only including R output that is necessary to answer the questions below (avoiding lengthy output).
    - providing short written answers explaining your work, and writing in complete sentences.


Due on Carmen Saturday, February 04 by 11:59 pm. All uploads must be .pdf.
Submissions will be accepted for 24 hours past this deadline, with a deduction of 1%
per hour. Absolutely no submissions will be accepted after this grace period.
\

**Concepts \& Application**

In this assignment, you will

* identify the mean and variance functions of of several probability distributions.
* find expectation and variances of several probability distributions.
* Finding MSE for estimators.
* Finding and showing unbiased estimators for parameters.


This homework is worth 40 points. 

This credit will be earned by:

Submitting both the **pdf** file and your completed **Rmd** file to Carmen: 2 points.

Problems completion : 38 points.

Total: 40 points

\vspace{3in}

**Question 1**

Let $f(y\mid\theta)=\frac{1}{\lambda+1}e^{\frac{-y}{\lambda+1}}$ for $y>0$ and $\lambda>-1$.


(a) Prove $E(y)=\lambda+1$. Use integration by by parts. *Hint*:Perhaps let $\alpha=\frac{1}{\lambda+1}$
(b)	Suppose an estimator $\hat{\lambda}$ for the parameter $\lambda$ will be the sample mean $\bar{y}$. (Here, $\hat{\lambda}=\bar{y}$). Compute the bias of the estimator $\hat{\lambda}$, that is B($\hat{\lambda}$)
(c) Propose an unbiased estimator for $\lambda$.



\

**Solution to Question 1**

Your answers go here.

**Part a:**

Integration by parts as suggested in the hint.
\[ \int \frac{1}{\lambda+1} \cdot e^{-\frac{y}{\lambda+1}} \, dy = -\frac{1}{\lambda+1} \cdot (\lambda+1)e^{-\frac{y}{\lambda+1}} - \int -(\lambda+1)e^{-\frac{y}{\lambda+1}} \, dy \]

Simplify.
\[ = 0 - [-(\lambda+1) \cdot (\lambda+1)] = (\lambda+1)^2 \]

Simplify to the final value which matches what we expected.
\[ E(y) = \lambda + 1 \]

**Part b:**

The bias formula as given.
\[ B(\hat{\lambda}) = E(\hat{\lambda}) - \lambda \]

We also know that \(\hat{\lambda} = \bar{y}\) so we need to find \(E(\bar{y})\). Since we know that \(E(y) = \lambda + 1 \) we can also determine that \(E(\bar{y}) = \lambda + 1\).

Therefore, the bias of the estimator is:
\[ B(\bar{y}) = E(\bar{y}) - \lambda = (\lambda + 1) - \lambda = 1 \]

**Part c:**

In order to get an unbiased estimator we basically need to 'cancel out' the bias in the given estimator. This leads to:
\(\tilde{\lambda} = \bar{y} - 1\)

\vspace{1cm}

**Question 2**

Consider a random sample $Y_1,Y_2,\cdots,Y_n\sim f(y\mid\beta)=\beta y^{\beta-1}$ for $0<y<1$ and $\beta>-1$.

(a) Show that $\bar{y}$ is an unbiased estimator for $\frac{\beta}{\beta+1}$.
(b)	Compute $E(y)^2$, $V(\bar{y})$.
(c) Compute $MSE(\bar{y})$, where $\bar{y}$ is the estimator for $\frac{\beta}{\beta+1}$.
\

**Solution to Question 2**

Your answers go here.

**Part a:**

The expected value of \( Y \) is given by:
\[ E(Y) = \int_{0}^{1} y \cdot \beta y^{\beta - 1} \, dy \]

\[ = \beta \int_{0}^{1} y^{\beta} \, dy \]
\[ = \beta \left[ \frac{y^{\beta + 1}}{\beta + 1} \right]_{0}^{1} \]
\[ = \frac{\beta}{\beta + 1} \]

Thus, \( \bar{y} \) is an unbiased estimator for \( \frac{\beta}{\beta+1} \) because the expectation of y would also be \( \bar{y} \) (the mean). Therefore there is no bias present.

**Part b:**
We have \( E(y) = \frac{\beta}{\beta+1} \). So, 

\[ E(y)^2 = \left( \frac{\beta}{\beta+1} \right)^2 = \frac{\beta^2}{(\beta+1)^2} \]

The variance of \( \bar{y} \) is given by:
\[ V(\bar{y}) = V \left( \frac{1}{n} \sum_{i=1}^{n} Y_i \right) = \frac{1}{n^2} \sum_{i=1}^{n} V(Y_i) \]

Simplifying to:
\[\frac{\beta}{n(\beta+1)^2(\beta+2)}\]

**Part c:**

The MSE of \( \bar{y} \) is given by:
\[ \text{MSE}(\bar{y}) = V(\bar{y}) + \left( B(\bar{y}) \right)^2 \]

We know \( V(\bar{y}) \) from part b, and we know that \( B(\bar{y}) = 0 \) since \( \bar{y} \) is an unbiased estimator. So, 
\[ \text{MSE}(\bar{y}) = \frac{\beta}{n(\beta+1)^2(\beta+2)} \]

\vspace{1cm}

**Question 3**

A business models the number of customers $C_i$ who visit on a given day as a Poisson random variable
with mean $\lambda$. A random sample $C_1,C_2,\cdots,C_n$ over $n$ days is taken. Here simply, $C_i\sim Poisson (\lambda)$. The profits $P_i$ associated with each customer
are $P_i=5C_i+C_i^2$. 
Since $P_i$ is a random variable, and it has its own expectation, $\mu_{P}$.

(a) Compute $E(C_i^2)$, using the known facts that for $C_i\sim Poisson (\lambda)$ with $E(C_i)=\lambda$ and $V(C_i)=\lambda$.
(b) Compute $E(P_i)$
(c) Compute $E(\bar{C}^2)$ using known facts about $E(\bar{C})$ and $V(\bar{C})$.
(d) Propose an unbiased estimator for $\mu_{P}$. *Hint:* it will be of the form $\hat{\mu_{P}}=a\bar{C}+b\bar{C}^2$, where $a$ and $b$ are constants.




\

**Solution to Question 3**

Your answers go here.

**Part a:**

Because \(C_i \sim \text{Poisson}(\lambda)\) we know that \(E(C_i) = \lambda\) and \(V(C_i) = \lambda\). Therefore know that \(E(C_i^2) = V(C_i) + [E(C_i)]^2\). This means,
\[E(C_i^2) = \lambda + \lambda^2\]

**Part b:**

Using the facts from the problem we can determine that, \[E(P_i) = E(5C_i + C_i^2) = 5E(C_i) + E(C_i^2)\]

Substituting the known values, \[E(P_i) = 5\lambda + (\lambda + \lambda^2)\].

Then simplifying we get,
\[E(P_i) = 6\lambda + \lambda^2\]

**Part c:**

Firstly we need to use that because \[V(\bar{C}) = [E(\bar{C})]^2- E(\bar{C}^2)\] we can determine that as \[E(\bar{C}^2) = V(\bar{C}) + [E(\bar{C})]^2\].

Then we find \(E(\bar{C}) = \lambda\) and \(V(\bar{C}) = \frac{\lambda}{n}\) using the properties of the poisson distribution.

Then finally we get: 
\[E(\bar{C}^2) = \frac{\lambda}{n} + \lambda^2\]

**Part d:** ## let a = 5 b =1

We need to find \(\hat{\mu_{P}}\) such that \(E(\hat{\mu_{P}}) = \mu_{P}\). 

We know that \(E(\bar{C}) = \lambda\) and \(E(\bar{C}^2) = \frac{\lambda}{n} + \lambda^2\), and based on the hint the result will be in the form, \[\hat{\mu_{P}} = a\bar{C} + b\bar{C}^2\] where \(a\) and \(b\) are constants. 

To make \(\hat{\mu_{P}}\) unbiased \[E(\hat{\mu_{P}}) = \mu_{P}\] must hold true. Also keeping in mind that E(P) = \(\mu_{P}\)

Substituting the expressions for \(\bar{C}\) and \(\bar{C}^2\), we get:

\[a\lambda + b\left(\frac{\lambda}{n} + \lambda^2\right) = 6\lambda + \lambda^2\]

Then solving for a and b we get \(a = 6 - \frac{1}{n}\) and \(b = 1\). Then we rexpress that in terms of \(\bar{C}\).

\[(6 - \frac{1}{n})\bar{C}+\bar{C}^2\]

\vspace{1cm}

**Question 4**


Consider an unknown parameter $\theta$. It can be estimated with either $\hat{\theta_1}$ with variance $V(\hat{\theta_1})=\sigma_1^2$ or, $\hat{\theta_2}$ with variance $V(\hat{\theta_2})=\sigma_2^2$. The estimators are correlated with $Cov(\hat{\theta_1},\hat{\theta_2})=\sigma_{12}$ , and, both $\hat{\theta_1}$ and $\hat{\theta_2}$ are
unbiased estimator for $\theta$.
Consider the unbiased estimator $\hat{\theta_3}=a\cdot\hat{\theta_1}+(1-a)\cdot\hat{\theta_2}$, where $a\in \mathbb{R}$. What value of $a$ minimizes the
variance of $\hat{\theta_3}$?

**Solution to Question 4**

Your answers go here.
\[ = a^2(\sigma_1^2 + \sigma_2^2 - 2\sigma_{12}) + \sigma_2^2 - 2a(\sigma_2^2 - \sigma_{12}) \]

Using the given properties of covariance, variance and what we discussed in class we can get to:

\[ = a^2(\sigma_1^2 + \sigma_2^2 - 2\sigma_{12}) + \sigma_2^2 - 2a(\sigma_2^2 - \sigma_{12}) \]

Now we take the derivative with respect to a and set it equal to 0. 

\[ \frac{d}{da} V(\hat{\theta_3}) = 2a(\sigma_1^2 + \sigma_2^2 - 2\sigma_{12}) - 2(\sigma_2^2 - \sigma_{12}) = 0 \]

Then we solve for a:
\[ a = \frac{\sigma_2^2 - \sigma_{12}}{\sigma_1^2 + \sigma_2^2 - 2\sigma_{12}} \]

Then we solve for a = 0.
\[ a = \frac{\sigma_2^2 - \sigma_{12}}{\sigma_1^2 + \sigma_2^2 - 2\sigma_{12}} \]
\vspace{1cm}

**Question 5**

Consider a sample of three observations $X_1,X_2,\cdots,X_n$ from a normal distribution with mean $\mu$ and variance $1$, where $\mu$ is unknown. That is $X_1,X_2,\cdots,X_n\sim N(\mu,1)$.
Consider two distinct estimators for $\mu$: 

$\hat{\mu_1}=\frac{1}{3}x_1+\frac{1}{3}x_2+\frac{1}{3}x_3$

$\hat{\mu_2}=\frac{1}{10}x_1+\frac{1}{10}x_2+\frac{1}{10}x_3$.

For what values of $\mu$ does $\hat{\mu_2}$
achieve a lower MSE than $\hat{\mu_1}$ (if any)?


**Solution to Question 5**
Your answers go here.

For the two estimators \(\hat{\mu_1}\) and \(\hat{\mu_2}\), the MSE can be written as:

\[MSE(\hat{\mu_1}) = E\left(\left(\frac{1}{3}x_1 + \frac{1}{3}x_2 + \frac{1}{3}x_3 - \mu\right)^2\right)\]

\[MSE(\hat{\mu_2}) = E\left(\left(\frac{1}{10}x_1 + \frac{1}{10}x_2 + \frac{1}{10}x_3 - \mu\right)^2\right)\]

Expanding the squares:

\[MSE(\hat{\mu_1}) = \frac{1}{9}E\left((x_1 - \mu)^2\right) + \frac{1}{9}E\left((x_2 - \mu)^2\right) + \frac{1}{9}E\left((x_3 - \mu)^2\right)\]

\[MSE(\hat{\mu_2}) = \frac{1}{100}E\left((x_1 - \mu)^2\right) + \frac{1}{100}E\left((x_2 - \mu)^2\right) + \frac{1}{100}E\left((x_3 - \mu)^2\right)\]

Given that \(X_1, X_2, X_3\) are i.i.d. samples from a normal distribution with mean \(\mu\) and variance \(1\), we have \(E\left((x_i - \mu)^2\right) = \text{Var}(X_i) = 1\).

\[MSE(\hat{\mu_1}) = \frac{1}{9} + \frac{1}{9} + \frac{1}{9} = \frac{1}{3}\]

\[MSE(\hat{\mu_2}) = \frac{1}{100} + \frac{1}{100} + \frac{1}{100} = \frac{3}{100}\]

So, for \(\hat{\mu_2}\) to achieve a lower MSE than \(\hat{\mu_1}\), we need:

\[MSE(\hat{\mu_2}) < MSE(\hat{\mu_1})\]

\[\frac{3}{100} < \frac{1}{3}\]

This condition holds for all values of \(\mu\). Therefore, \(\hat{\mu_2}\) achieves a lower MSE than \(\hat{\mu_1}\) for any value of \(\mu\).
